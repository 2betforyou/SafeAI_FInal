{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["aHkV_hZG50MN","m2LiQ5zN54ea"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"xtO31s489w7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749795691540,"user_tz":-540,"elapsed":4567,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"08ec736b-b4e3-4fee-d591-f5781e808595"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip check"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fn6Kb9a9fp7q","executionInfo":{"status":"ok","timestamp":1749795386903,"user_tz":-540,"elapsed":1768,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"7f989829-2926-4c73-e3fc-4ec7043567be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ipython 7.34.0 requires jedi, which is not installed.\n","torch 2.6.0+cu124 has requirement nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2.\n","torch 2.6.0+cu124 has requirement nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82.\n","torch 2.6.0+cu124 has requirement nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82.\n","torch 2.6.0+cu124 has requirement nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82.\n","torch 2.6.0+cu124 has requirement nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75.\n","torch 2.6.0+cu124 has requirement nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61.\n","torch 2.6.0+cu124 has requirement nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82.\n","torch 2.6.0+cu124 has requirement nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83.\n","torch 2.6.0+cu124 has requirement nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3.\n","torch 2.6.0+cu124 has requirement nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82.\n"]}]},{"cell_type":"markdown","source":["#### Prepare Asian Images (BioID-multi-race가 없다면)"],"metadata":{"id":"aHkV_hZG50MN"}},{"cell_type":"code","source":["!pip install -q dlib opencv-python\n","\n","!wget -O predictor.bz2 http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","!bzip2 -dk predictor.bz2\n","\n","import os\n","os.rename(\"predictor\", \"shape_predictor_68_face_landmarks.dat\")\n","\n","# data.zip\n","from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kNf8-RBmNRhV","executionInfo":{"status":"error","timestamp":1749795394893,"user_tz":-540,"elapsed":7974,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"0a3e54cf-ad9f-438a-ffda-3e79446ab5b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mTraceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n","    status = run_func(*args)\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n","    return func(self, options, args)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n","    conflicts = self._determine_conflicts(to_install)\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n","    return check_install_conflicts(to_install)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n","    package_set, _ = create_package_set_from_installed()\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n","    dependencies = list(dist.iter_dependencies())\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 222, in iter_dependencies\n","    req = Requirement(req_string.strip())\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n","    parsed = _parse_requirement(requirement_string)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n","    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n","                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_tokenizer.py\", line 104, in __init__\n","    self.rules: dict[str, re.Pattern[str]] = {\n","                                             ^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/packaging/_tokenizer.py\", line 104, in <dictcomp>\n","    self.rules: dict[str, re.Pattern[str]] = {\n","                                             ^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 10, in <module>\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n","    return command.main(cmd_args)\n","           ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n","    return self._main(args)\n","           ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n","    return run(options, args)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n","    logger.debug(\"Exception information:\", exc_info=True)\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1477, in debug\n","    self._log(DEBUG, msg, args, **kwargs)\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1634, in _log\n","    self.handle(record)\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1644, in handle\n","    self.callHandlers(record)\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1706, in callHandlers\n","    hdlr.handle(record)\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 978, in handle\n","    self.emit(record)\n","  File \"/usr/lib/python3.11/logging/handlers.py\", line 75, in emit\n","    logging.FileHandler.emit(self, record)\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1230, in emit\n","    StreamHandler.emit(self, record)\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 1110, in emit\n","    msg = self.format(record)\n","          ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 953, in format\n","    return fmt.format(record)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n","    formatted = super().format(record)\n","                ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 695, in format\n","    record.exc_text = self.formatException(record.exc_info)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/logging/__init__.py\", line 645, in formatException\n","    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n","  File \"/usr/lib/python3.11/traceback.py\", line 124, in print_exception\n","    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/traceback.py\", line 728, in __init__\n","    self.stack = StackSummary._extract_from_extended_frame_gen(\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/traceback.py\", line 425, in _extract_from_extended_frame_gen\n","    result.append(FrameSummary(\n","                  ^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/traceback.py\", line 261, in __init__\n","    def __init__(self, filename, lineno, name, *, lookup_line=True,\n","\n","KeyboardInterrupt\n","^C\n","--2025-06-13 06:16:32--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","Resolving dlib.net (dlib.net)... 107.180.26.78\n","Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 [following]\n","--2025-06-13 06:16:32--  https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","Connecting to dlib.net (dlib.net)|107.180.26.78|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 64040097 (61M)\n","Saving to: ‘predictor.bz2’\n","\n","predictor.bz2       100%[===================>]  61.07M  41.0MB/s    in 1.5s    \n","\n","2025-06-13 06:16:34 (41.0 MB/s) - ‘predictor.bz2’ saved [64040097/64040097]\n","\n","\n","bzip2: Control-C or similar caught, quitting.\n","bzip2: Deleting output file predictor, if it exists.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'predictor' -> 'shape_predictor_68_face_landmarks.dat'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2340013170>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# data.zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predictor' -> 'shape_predictor_68_face_landmarks.dat'"]}]},{"cell_type":"code","source":["import zipfile, os\n","\n","zip_path = next(iter(uploaded))\n","extract_dir = \"data\"\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_dir)\n","\n","import cv2\n","import dlib\n","import numpy as np\n","\n","predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","detector = dlib.get_frontal_face_detector()\n","\n","output_dir = \"output_384x286\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","from glob import glob\n","from tqdm import tqdm\n","\n","img_paths = sorted(glob(f\"{extract_dir}/**/*.jpg\", recursive=True))\n","\n","for path in tqdm(img_paths):\n","    name = os.path.splitext(os.path.basename(path))[0]\n","    img = cv2.imread(path)\n","    if img is None:\n","        continue\n","\n","    resized = cv2.resize(img, (384, 286))\n","    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n","\n","    dets = detector(gray, 1)\n","    if len(dets) == 0:\n","        continue\n","\n","    shape = predictor(gray, dets[0])\n","    landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n","\n","    left_eye = landmarks[36:42]\n","    right_eye = landmarks[42:48]\n","\n","    left_center = np.mean(left_eye, axis=0)\n","    right_center = np.mean(right_eye, axis=0)\n","\n","    cv2.imwrite(f\"{output_dir}/{name}.jpg\", gray)\n","    with open(f\"{output_dir}/{name}.eye\", \"w\") as f:\n","        f.write(f\"{left_center[0]:.2f} {left_center[1]:.2f} {right_center[0]:.2f} {right_center[1]:.2f}\\n\")\n","\n","!zip -r output_384x286.zip output_384x286\n","files.download(\"output_384x286.zip\")"],"metadata":{"id":"x_sH7MKVPIsg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, shutil, cv2\n","from glob import glob\n","\n","scut_dir = \"output_384x286\"\n","bioid_dir = \"/content/drive/MyDrive/BioID-FaceDatabase-V1.2\"\n","merged_dir = \"BioID-multi-race\"\n","\n","os.makedirs(merged_dir, exist_ok=True)\n","\n","bioid_pgms = glob(f\"{bioid_dir}/**/*.pgm\", recursive=True)\n","bioid_eyes = glob(f\"{bioid_dir}/**/*.eye\", recursive=True)\n","\n","for f in bioid_pgms + bioid_eyes:\n","    shutil.copy(f, os.path.join(merged_dir, os.path.basename(f)))\n","\n","scut_jpgs = glob(f\"{scut_dir}/*.jpg\")\n","\n","for jpg_path in scut_jpgs:\n","    base = os.path.splitext(os.path.basename(jpg_path))[0]\n","\n","    img = cv2.imread(jpg_path, cv2.IMREAD_GRAYSCALE)\n","    if img is None:\n","        continue\n","\n","    pgm_out = os.path.join(merged_dir, base + \".pgm\")\n","    cv2.imwrite(pgm_out, img)\n","\n","    eye_src = os.path.join(scut_dir, base + \".eye\")\n","    eye_dst = os.path.join(merged_dir, base + \".eye\")\n","    if os.path.exists(eye_src):\n","        shutil.copy(eye_src, eye_dst)\n","\n","print(f\"Merged: {len(os.listdir(merged_dir))} files in '{merged_dir}/'\")"],"metadata":{"id":"nwrMAvVwRKfz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W_uIzC78KCwv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 이미 추가 되었다면(BioID-multi-race), 여기서부터"],"metadata":{"id":"m2LiQ5zN54ea"}},{"cell_type":"code","source":["import shutil\n","import os\n","\n","drive_target_dir = \"/content/drive/MyDrive/BioID-multi-race\"\n","\n","if os.path.exists(drive_target_dir):\n","    shutil.rmtree(drive_target_dir)\n","\n","shutil.copytree(\"BioID-multi-race\", drive_target_dir)\n","\n","print(\"EndSave\")"],"metadata":{"id":"hhbRpXQKRqd6","colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"status":"error","timestamp":1749795427046,"user_tz":-540,"elapsed":26425,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"6a26fed2-8fd5-4f52-dd76-6a08b51bf2ce"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'BioID-multi-race'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-9213216>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_target_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BioID-multi-race\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrive_target_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EndSave\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m    570\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutil.copytree\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'BioID-multi-race'"]}]},{"cell_type":"code","source":["!pip install -q opencv-python dlib\n","\n","import os, cv2, struct, zipfile\n","import numpy as np\n","import dlib\n","from glob import glob\n","from tqdm import tqdm\n","from google.colab import files\n","\n","\n","# data.zip\n","uploaded = files.upload()"],"metadata":{"id":"Nq8d1o4uxJO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["zip_filename = next(iter(uploaded))\n","\n","extract_dir = \"data\"\n","with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n","    zip_ref.extractall(extract_dir)\n","\n","predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","detector = dlib.get_frontal_face_detector()\n","\n","def read_scaled_pts(path, img_shape, target_shape=(384, 286)):\n","    with open(path, 'rb') as f:\n","        raw = f.read()\n","    pts = struct.unpack('f' * 172, raw)\n","    pts = np.array(pts).reshape((86, 2))\n","    h0, w0 = img_shape\n","    w1, h1 = target_shape\n","    scale_x, scale_y = w1 / w0, h1 / h0\n","    return pts * [scale_x, scale_y]\n","\n","out_dir = \"output\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","img_paths = sorted(glob(f\"{extract_dir}/*.jpg\"))\n","for img_path in tqdm(img_paths):\n","    name = os.path.splitext(os.path.basename(img_path))[0]\n","    pts_path = os.path.join(extract_dir, name + \".pts\")\n","    if not os.path.exists(pts_path):\n","        continue\n","\n","    img = cv2.imread(img_path)\n","    if img is None:\n","        continue\n","    orig_h, orig_w = img.shape[:2]\n","    resized = cv2.resize(img, (384, 286))\n","    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n","\n","    pts = read_scaled_pts(pts_path, (orig_h, orig_w))\n","    left_eye = pts[36:42]\n","    right_eye = pts[42:48]\n","    left_center = np.mean(left_eye, axis=0)\n","    right_center = np.mean(right_eye, axis=0)\n","\n","    y_diff = left_center[1] - right_center[1]\n","    if abs(y_diff) > 15:\n","        dets = detector(gray, 1)\n","        if len(dets) > 0:\n","            shape = predictor(gray, dets[0])\n","            landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n","            left_center = np.mean(landmarks[36:42], axis=0)\n","            right_center = np.mean(landmarks[42:48], axis=0)\n","\n","    with open(os.path.join(out_dir, name + \".eye\"), \"w\") as f:\n","        f.write(f\"{left_center[0]:.2f} {left_center[1]:.2f} {right_center[0]:.2f} {right_center[1]:.2f}\\n\")\n","\n","    pgm_path = os.path.join(out_dir, name + \".pgm\")\n","    cv2.imwrite(pgm_path, gray)"],"metadata":{"id":"1iT4nMs9TXyi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 모델 학습"],"metadata":{"id":"YS42kNew5_In"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"metadata":{"id":"VZCbG23sYJse","executionInfo":{"status":"ok","timestamp":1749795718680,"user_tz":-540,"elapsed":5029,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# # base_path = \"/content/drive/MyDrive/SafeAI/BioID-FaceDatabase-V1.2\"\n","# base_path = \"/content/drive/MyDrive/BioID-multi-race\"\n","# image_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".pgm\")])\n","# label_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".eye\")])\n","\n","# print(len(image_files))\n","# print(len(label_files))"],"metadata":{"id":"izbzPEhAY4iU","executionInfo":{"status":"ok","timestamp":1749795726600,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# base_path = \"/content/drive/MyDrive/SafeAI_Final/BioID-multi-race\"\n","base_path = \"/content/drive/MyDrive/BioID-multi-race\"\n","image_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".pgm\")])\n","label_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".eye\")])\n","\n","print(len(image_files))\n","print(len(label_files))\n","\n","data = []\n","for img_file, label_file in zip(image_files, label_files):\n","    img_path = os.path.join(base_path, img_file)\n","    label_path = os.path.join(base_path, label_file)\n","\n","    with open(label_path, \"r\") as f:\n","        lines = f.readlines()\n","        if len(lines) == 1:\n","            lx, ly, rx, ry = map(float, lines[0].strip().split())\n","        else:\n","          lx, ly, rx, ry = map(float, lines[1].strip().split())\n","        # print(f\"left: {lx}, {ly} / right: {rx}, {ry}\")\n","        data.append((img_path, [lx, ly, rx, ry]))\n"],"metadata":{"id":"3ZrXlwmuY7Kl","executionInfo":{"status":"ok","timestamp":1749795892499,"user_tz":-540,"elapsed":96133,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c83b3606-17c6-4c78-9565-b52cd56f7fbc"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["2945\n","2945\n"]}]},{"cell_type":"code","source":["len(data)"],"metadata":{"id":"polwZbK03w8R","executionInfo":{"status":"ok","timestamp":1749795892539,"user_tz":-540,"elapsed":16,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"efb7689e-61f7-4c4e-8855-1b7de83f4a21"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2945"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["random.seed(42)\n","random.shuffle(data)\n","\n","holdout_data = data[:21]    # 비교용\n","remaining_data = data[21:]  # 학습용\n","\n","df = pd.DataFrame(remaining_data, columns=[\"image_path\", \"eye_coords\"])\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","holdout_df = pd.DataFrame(holdout_data, columns=[\"image_path\", \"eye_coords\"])\n","\n","print(len(remaining_data))"],"metadata":{"id":"kKm_Kj7UtFPA","executionInfo":{"status":"ok","timestamp":1749795892540,"user_tz":-540,"elapsed":6,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a71e8049-889a-4488-9495-ae91cfa62610"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["2924\n"]}]},{"cell_type":"code","source":["def preprocess_image(image_path):\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","    image = image.astype(np.float32) / 255.0\n","    image = np.expand_dims(image, axis=0)  # (1, H, W)\n","    return image  # (1, 286, 384)"],"metadata":{"id":"qtRzm7nW6zx4","executionInfo":{"status":"ok","timestamp":1749795892542,"user_tz":-540,"elapsed":1,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class EyeDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.df = dataframe.reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        image = preprocess_image(row[\"image_path\"])  # (1, 128, 128)\n","        label = np.array(row[\"eye_coords\"], dtype=np.float32)\n","        return torch.tensor(image), torch.tensor(label)"],"metadata":{"id":"183Lw50oYF7g","executionInfo":{"status":"ok","timestamp":1749795892545,"user_tz":-540,"elapsed":2,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["class HoldoutDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.df = dataframe.reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        image = preprocess_image(row[\"image_path\"])  # (1, H, W)\n","        label = np.array(row[\"eye_coords\"], dtype=np.float32)\n","        return torch.tensor(image), torch.tensor(label)"],"metadata":{"id":"mwwAX24oy-Bx","executionInfo":{"status":"ok","timestamp":1749795892546,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class EyeCNN(nn.Module):\n","    def __init__(self):\n","        super(EyeCNN, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),  # (286, 384) → (143, 192)\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),  # (143, 192) → (71, 96)\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),  # (71, 96) → (35, 48)\n","\n","            nn.AdaptiveAvgPool2d((4, 6)), # 128 * 4 * 6 = 3072\n","            nn.Flatten(),\n","            nn.Dropout(0.3),\n","            nn.Linear(3072, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 4)  # [lx, ly, rx, ry]\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"QIb5l3zryhF3","executionInfo":{"status":"ok","timestamp":1749795892567,"user_tz":-540,"elapsed":20,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"mvffs_HBfSg_","executionInfo":{"status":"ok","timestamp":1749795892572,"user_tz":-540,"elapsed":4,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e37d32a0-2684-4ceb-8142-ec59b25ded21"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["train_dataset = EyeDataset(train_df)\n","test_dataset = EyeDataset(test_df)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","model = EyeCNN().to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"9_HOiKRCYNrD","executionInfo":{"status":"ok","timestamp":1749795892573,"user_tz":-540,"elapsed":1,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["#### Training"],"metadata":{"id":"eaFqJo4F69oU"}},{"cell_type":"code","source":["epochs = 250\n","best_loss = float('inf')\n","best_model_state = None\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        preds = model(images)\n","        loss = criterion(preds, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n","\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        best_model_state = model.state_dict()\n","\n","\n","model.load_state_dict(best_model_state)"],"metadata":{"id":"p24Debyv6zt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(best_loss)"],"metadata":{"id":"6a8FKoh2txT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, dataset, device):\n","    model.eval()\n","    predictions = []\n","\n","    for i in range(len(dataset)):\n","        image_tensor, _ = dataset[i]\n","        image_input = image_tensor.unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            pred = model(image_input).cpu().numpy()[0]\n","        predictions.append(pred.tolist())\n","\n","    return predictions"],"metadata":{"id":"7KhDGWeb4hFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_cnn_predictions(dataset, predictions, num_samples=21):\n","    data_pairs = []\n","\n","    for i in range(min(num_samples, len(dataset))):\n","        _, label = dataset[i]\n","        pred = predictions[i]\n","\n","        gt = label.numpy().tolist()\n","        pr = np.round(pred, 2).tolist()\n","\n","        print(f\"샘플 {i+1}:\")\n","        print(f\" - 정답 좌표 (GT):     {label.numpy().tolist()}\")\n","        print(f\" - 예측 좌표 (CNN):    {np.round(pred, 2).tolist()}\")\n","        print()\n","\n","        data_pairs.append((gt, pr))\n","\n","    mae_list, mse_list, euclidean_list = [], [], []\n","\n","    for gt, pr in data_pairs:\n","        gt = np.array(gt)\n","        pr = np.array(pr)\n","        diff = gt - pr\n","\n","        mae = np.mean(np.abs(diff))\n","        mse = np.mean(diff ** 2)\n","        euclidean = np.linalg.norm(diff)\n","\n","        mae_list.append(mae)\n","        mse_list.append(mse)\n","        euclidean_list.append(euclidean)\n","\n","    print(f\"MAE: {np.mean(mae_list):.2f}\")\n","    print(f\"MSE: {np.mean(mse_list):.2f}\")\n","    print(f\"Euclidean: {np.mean(euclidean_list):.2f}\")"],"metadata":{"id":"qtWCgn-24gzZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_predictions(dataset, predictions, num_samples=21, cols=3):\n","    rows = (num_samples + cols - 1) // cols\n","    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n","\n","    for i in range(min(num_samples, len(dataset))):\n","        img_tensor, label = dataset[i]\n","        img = img_tensor.numpy()[0]\n","        img = np.squeeze(img)\n","        pred = predictions[i]\n","\n","        ax = axes[i // cols][i % cols] if rows > 1 else axes[i]\n","        ax.imshow(img, cmap='gray')\n","        ax.scatter(label[0], label[1], color='blue', label='GT Left')\n","        ax.scatter(label[2], label[3], color='blue', label='GT Right')\n","        ax.scatter(pred[0], pred[1], color='red', marker='x', label='Pred Left')\n","        ax.scatter(pred[2], pred[3], color='red', marker='x', label='Pred Right')\n","        ax.axis('off')\n","        if i == 0:\n","            ax.legend(loc='upper right')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"dx9wm5WM49vm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["holdout_dataset = HoldoutDataset(holdout_df)"],"metadata":{"id":"7tbMSd385J8_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = predict(model, holdout_dataset, device)"],"metadata":{"id":"xWEAgESZ5Dm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_cnn_predictions(holdout_dataset, predictions)"],"metadata":{"id":"Pq378O4c5DgP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 결과 시각화"],"metadata":{"id":"O7bG02Tw6FIo"}},{"cell_type":"code","source":["visualize_predictions(holdout_dataset, predictions, 21, 3)"],"metadata":{"id":"DhesTDY15DW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/SafeAI_Final/eye_cnn_250epochs.pth\")"],"metadata":{"id":"9yCEIDishjhg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Machine Unlearning: Unlearn Asian Images"],"metadata":{"id":"U6OR83zn6H2C"}},{"cell_type":"code","source":["# 학습한 모델 로드\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","loaded_model = EyeCNN().to(device)\n","loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/SafeAI_Final/eye_cnn_250epochs.pth\"))\n","loaded_model.eval()\n"],"metadata":{"id":"vCVI4cNC0fC4","executionInfo":{"status":"ok","timestamp":1749795975822,"user_tz":-540,"elapsed":34,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1088a10e-97b3-41ba-b08f-19a28f894038"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["EyeCNN(\n","  (model): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU()\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU()\n","    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (12): AdaptiveAvgPool2d(output_size=(4, 6))\n","    (13): Flatten(start_dim=1, end_dim=-1)\n","    (14): Dropout(p=0.3, inplace=False)\n","    (15): Linear(in_features=3072, out_features=256, bias=True)\n","    (16): ReLU()\n","    (17): Linear(in_features=256, out_features=4, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["device"],"metadata":{"id":"24TBUHnLrFxl","executionInfo":{"status":"ok","timestamp":1749795977403,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"122701bb-69cb-496a-8aee-9c42448cbed6"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["import os\n","import random\n","\n","base_path = \"/content/drive/MyDrive/BioID-multi-race\"\n","image_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".pgm\")])\n","\n","asian_fnames = {f for f in image_files if ('AM' in f or 'AF' in f)}\n","print(f\"폴더 기준 동양인 이미지 수: {len(asian_fnames)} / 전체: {len(image_files)}\")\n","\n","train_df = train_df.reset_index(drop=True)\n","test_df  = test_df.reset_index(drop=True)\n","\n","asian_sample_indices = train_df.index[\n","    train_df[\"image_path\"].apply(lambda p: os.path.basename(p)).isin(asian_fnames)\n","].tolist()\n","\n","print(f\"train_df 동양인 인덱스 수: {len(asian_sample_indices)} / 전체: {len(train_df)}\")\n","\n","train_dataset = EyeDataset(train_df)\n","test_dataset  = EyeDataset(test_df)\n","print(f\"train_dataset 길이: {len(train_dataset)}\")\n","\n","valid_indices = [idx for idx in asian_sample_indices if 0 <= idx < len(train_dataset)]\n","print(f\"유효한 동양인 인덱스 수: {len(valid_indices)}\")\n","\n","random.seed(42)\n","asian_holdout_indices = random.sample(valid_indices, 140)\n","\n","asian_unlearn_indices = [idx for idx in valid_indices\n","                         if idx not in asian_holdout_indices]\n","\n","print(f\"hold-out 140장: {len(asian_holdout_indices)}\")\n","print(f\"언러닝 대상 1000장: {len(asian_unlearn_indices)}\")\n"],"metadata":{"id":"iKuIQe6c0kjN","executionInfo":{"status":"ok","timestamp":1749795978298,"user_tz":-540,"elapsed":90,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9eaabd32-f713-4b06-e983-16269f773823"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["폴더 기준 동양인 이미지 수: 1424 / 전체: 2945\n","train_df 동양인 인덱스 수: 1140 / 전체: 2339\n","train_dataset 길이: 2339\n","유효한 동양인 인덱스 수: 1140\n","hold-out 140장: 140\n","언러닝 대상 1000장: 1000\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Subset\n","import time"],"metadata":{"id":"2jzcnk9O5ey4","executionInfo":{"status":"ok","timestamp":1749795980154,"user_tz":-540,"elapsed":4,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["def machine_unlearning_gradient_ascent(model, forget_loader, retain_loader, lr=5e-7, epochs=10):\n","    print(f\"[Stage 1] Gradient Ascent Unlearning\")\n","    print(f\"Learning rate: {lr}, Epochs: {epochs}\")\n","    print(f\"Forget samples: {len(forget_loader.dataset)}, Retain samples: {len(retain_loader.dataset)}\")\n","\n","    for epoch in range(epochs):\n","        # Forget phase\n","        model.train()\n","        epoch_forget_loss = 0\n","        for batch_idx, (data, targets) in enumerate(forget_loader):\n","            data, targets = data.to(device), targets.to(device)\n","            model.zero_grad()\n","\n","            predictions = model(data)\n","            forget_loss = F.mse_loss(predictions, targets)\n","            # forget_loss = torch.clamp(forget_loss, max=20)\n","\n","            if epoch == 0 and batch_idx == 0:\n","                print(f\"[DEBUG] Forget Loss (Epoch {epoch}, Batch {batch_idx}): {forget_loss.item():.4f}\")\n","\n","            forget_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","\n","            for param in model.parameters():\n","                if param.grad is not None:\n","                    param.data += lr * 60 * param.grad  # Gradient Ascent\n","\n","            epoch_forget_loss += forget_loss.item()\n","\n","        # Retain phase\n","        model.eval()\n","        epoch_retain_loss = 0\n","        for batch_idx, (data, targets) in enumerate(retain_loader):\n","            data, targets = data.to(device), targets.to(device)\n","            model.zero_grad()\n","            with torch.no_grad():\n","                _ = model(data)\n","\n","            model.train()\n","            predictions = model(data)\n","            retain_loss = F.mse_loss(predictions, targets)\n","\n","            retain_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n","\n","            for param in model.parameters():\n","                if param.grad is not None:\n","                    param.data -= lr * 6 * param.grad  # Gradient Descent\n","\n","            epoch_retain_loss += retain_loss.item()\n","\n","        print(f\"Epoch {epoch+1}: Forget Loss {epoch_forget_loss / len(forget_loader):.4f}, Retain Loss {epoch_retain_loss / len(retain_loader):.4f}\")\n","\n","    print(f\"Stage 1 completed\\n\")\n","    return model\n"],"metadata":{"id":"0TPVhjahMB5D","executionInfo":{"status":"ok","timestamp":1749795921095,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def ft(model, retain_loader, ft_lr=1e-6, ft_epochs=3):\n","    print(f\"[Fine-Tuning] Retain Set\")\n","    optimizer = torch.optim.Adam(model.parameters(), lr=ft_lr)\n","\n","    for epoch in range(ft_epochs):\n","        model.train()\n","        total_loss = 0\n","        for data, targets in retain_loader:\n","            data, targets = data.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","            predictions = model(data)\n","            loss = F.mse_loss(predictions, targets)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        print(f\"FT Epoch {epoch+1}: Loss {total_loss / len(retain_loader):.4f}\")\n","    print(\"Fine-Tuning Completed\\n\")\n","    return model\n"],"metadata":{"id":"fZSEzkQbLwyC","executionInfo":{"status":"ok","timestamp":1749795923151,"user_tz":-540,"elapsed":1,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def complete_unlearning_pipeline(model, dataset, asian_indices):\n","    \"\"\"Complete machine unlearning pipeline with minimal logging\"\"\"\n","    print(f\"Machine Unlearning Pipeline Started\")\n","    print(f\"Total data: {len(dataset)}, Asian samples: {len(asian_indices)}\")\n","    print(f\"Removal ratio: {len(asian_indices)/len(dataset)*100:.1f}%\\n\")\n","\n","    # Data splitting\n","    asian_data = Subset(dataset, asian_indices)\n","    western_indices = [i for i in range(len(dataset)) if i not in asian_indices]\n","    western_data = Subset(dataset, western_indices)\n","\n","    asian_loader = DataLoader(asian_data, batch_size=32, shuffle=True)\n","    western_loader = DataLoader(western_data, batch_size=32, shuffle=True)\n","\n","    # Stage 1: Gradient ascent unlearning\n","    model = machine_unlearning_gradient_ascent(model, asian_loader, western_loader, epochs=5)\n","\n","    # Stage 2: Fine tuning\n","    model = ft(model, western_loader)\n","\n","    print(f\"Machine Unlearning Pipeline Completed\")\n","    print(f\"Processed {len(asian_indices)} Asian samples\")\n","\n","    return model\n"],"metadata":{"id":"bCZEuu7G82N-","executionInfo":{"status":"ok","timestamp":1749795924144,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["if len(valid_indices) > 0:\n","    unlearned_model = complete_unlearning_pipeline(\n","        loaded_model,\n","        train_dataset,\n","        asian_unlearn_indices\n","    )\n","    print(\"Success\")\n","else:\n","    print(\"Index Out of Range\")\n"],"metadata":{"id":"GnEZ9FKE2e-J","executionInfo":{"status":"error","timestamp":1749796009280,"user_tz":-540,"elapsed":26040,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"colab":{"base_uri":"https://localhost:8080/","height":507},"outputId":"eb3269b9-6d29-40af-d84a-0710317b8c38"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Machine Unlearning Pipeline Started\n","Total data: 2339, Asian samples: 1000\n","Removal ratio: 42.8%\n","\n","[Stage 1] Gradient Ascent Unlearning\n","Learning rate: 5e-07, Epochs: 5\n","Forget samples: 1000, Retain samples: 1339\n","[DEBUG] Forget Loss (Epoch 0, Batch 0): 24.7570\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-3039952350>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     unlearned_model = complete_unlearning_pipeline(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mloaded_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0masian_unlearn_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-245105916>\u001b[0m in \u001b[0;36mcomplete_unlearning_pipeline\u001b[0;34m(model, dataset, asian_indices)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Stage 1: Gradient ascent unlearning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmachine_unlearning_gradient_ascent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masian_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwestern_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Stage 2: Fine tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-644427887>\u001b[0m in \u001b[0;36mmachine_unlearning_gradient_ascent\u001b[0;34m(model, forget_loader, retain_loader, lr, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mepoch_forget_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-1111494621>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, 128, 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eye_coords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-2903206852>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m  \u001b[0;31m# (1, 286, 384)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# 결과 저장\n","torch.save(unlearned_model.state_dict(), '/content/drive/MyDrive/SafeAI_Final/eye_cnn_unlearned.pth')\n","print(\"Model Saved: 'eye_cnn_unlearned.pth'\")\n"],"metadata":{"id":"vIf_A02Dj46G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터에서 동양인/서양인 인덱스 추출\n","test_asian_indices = []\n","test_western_indices = []\n","\n","for idx, row in test_df.iterrows():\n","    image_name = os.path.basename(row[\"image_path\"])\n","    if 'AM' in image_name or 'AF' in image_name:\n","        test_asian_indices.append(idx)\n","    else:\n","        test_western_indices.append(idx)\n","\n","print(f\"테스트 동양인 이미지: {len(test_asian_indices)}개\")\n","print(f\"테스트 서양인 이미지: {len(test_western_indices)}개\")\n"],"metadata":{"id":"6vNHAs_O5w8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Subset, DataLoader\n","\n","# 동양인/서양인 테스트 데이터셋 생성\n","test_asian_dataset = Subset(test_dataset, test_asian_indices)\n","test_western_dataset = Subset(test_dataset, test_western_indices)\n","\n","# 데이터로더 생성\n","test_asian_loader = DataLoader(test_asian_dataset, batch_size=32, shuffle=False)\n","test_western_loader = DataLoader(test_western_dataset, batch_size=32, shuffle=False)\n"],"metadata":{"id":"77j487Fz55j9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_unlearning_effectiveness(original_model, unlearned_model, test_asian_loader, test_western_loader):\n","    \"\"\"Evaluate unlearning effectiveness\"\"\"\n","\n","    device = next(original_model.parameters()).device\n","\n","    def calculate_performance(model, data_loader):\n","        model.eval()\n","        total_loss = 0\n","        with torch.no_grad():\n","            for data, targets in data_loader:\n","                data, targets = data.to(device), targets.to(device)\n","                predictions = model(data)\n","                loss = F.mse_loss(predictions, targets)\n","                total_loss += loss.item()\n","        return total_loss / len(data_loader)\n","\n","    # Original model performance\n","    orig_asian_loss = calculate_performance(original_model, test_asian_loader)\n","    orig_western_loss = calculate_performance(original_model, test_western_loader)\n","\n","    # Unlearned model performance\n","    unlearn_asian_loss = calculate_performance(unlearned_model, test_asian_loader)\n","    unlearn_western_loss = calculate_performance(unlearned_model, test_western_loader)\n","\n","    print(f\"Asian data performance change: {orig_asian_loss:.4f} -> {unlearn_asian_loss:.4f}\")\n","    print(f\"Western data performance change: {orig_western_loss:.4f} -> {unlearn_western_loss:.4f}\")\n","\n","    # Calculate unlearning effect ratios\n","    unlearning_ratio = unlearn_asian_loss / orig_asian_loss\n","    retention_ratio = unlearn_western_loss / orig_western_loss\n","\n","    return unlearning_ratio, retention_ratio\n"],"metadata":{"id":"hb4xfQHVRPGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원본 모델 로드 (언러닝 전)\n","original_model = EyeCNN().to(device)\n","original_model.load_state_dict(torch.load('/content/drive/MyDrive/SafeAI_Final/eye_cnn_250epochs.pth'))\n","original_model.eval()\n","\n","unlearning_ratio, retention_ratio = evaluate_unlearning_effectiveness(\n","    original_model=original_model,\n","    unlearned_model=unlearned_model,\n","    test_asian_loader=test_asian_loader,\n","    test_western_loader=test_western_loader\n",")"],"metadata":{"id":"WTYFzaxb5655"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","def visualize_multiple_predictions(images, labels, orig_preds, unlearned_preds, count=5, save_dir=\"prediction_outputs\"):\n","    images = images[:count].cpu()\n","    labels = labels[:count].cpu().numpy()\n","    orig_preds = orig_preds[:count].cpu().numpy()\n","    unlearned_preds = unlearned_preds[:count].cpu().numpy()\n","\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    for i in range(count):\n","        fig, ax = plt.subplots(figsize=(4, 4))\n","        img = images[i].squeeze(0).numpy()\n","        ax.imshow(img, cmap='gray')\n","        ax.axis('off')\n","        ax.set_title(f\"Sample {i}\")\n","\n","        lx_gt, ly_gt, rx_gt, ry_gt = labels[i]\n","        lx_u, ly_u, rx_u, ry_u = unlearned_preds[i]\n","\n","        ax.scatter([lx_gt, rx_gt], [ly_gt, ry_gt], color='blue', label='GT', s=30)\n","        ax.scatter([lx_u, rx_u], [ly_u, ry_u], color='red', marker='x', label='Unlearned', s=40)\n","        ax.plot([lx_gt, lx_u], [ly_gt, ly_u], '--', color='red', linewidth=1)\n","        ax.plot([rx_gt, rx_u], [ry_gt, ry_u], '--', color='red', linewidth=1)\n","\n","        left_err = np.linalg.norm([lx_u - lx_gt, ly_u - ly_gt])\n","        right_err = np.linalg.norm([rx_u - rx_gt, ry_u - ry_gt])\n","        ax.text(lx_u + 5, ly_u, f\"{left_err:.1f}px\", color='red', fontsize=8)\n","        ax.text(rx_u + 5, ry_u, f\"{right_err:.1f}px\", color='red', fontsize=8)\n","\n","        ax.legend(loc='lower right', fontsize=8)\n","\n","        save_path = os.path.join(save_dir, f\"sample_{i}.png\")\n","        plt.savefig(save_path, bbox_inches='tight')\n","        plt.close()\n","        print(f\"[✓] 저장됨: {save_path}\")"],"metadata":{"id":"E1WxAuLdkAgS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"ca0SyPkCugzW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_iter = iter(test_asian_loader)\n","images, labels = next(data_iter)\n","\n","images = images.to(device)\n","labels = labels.to(device)\n","\n","with torch.no_grad():\n","    orig_preds = original_model(images)\n","    unlearned_preds = unlearned_model(images)\n","\n","visualize_multiple_predictions(\n","    images, labels, orig_preds, unlearned_preds,\n","    count=20,\n","    save_dir=\"/content/drive/MyDrive/SafeAI_Final/saved_predictions1\"\n",")"],"metadata":{"id":"vd06ScQtkB4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_iter = iter(test_western_loader)\n","images, labels = next(data_iter)\n","\n","images = images.to(device)\n","labels = labels.to(device)\n","\n","with torch.no_grad():\n","    orig_preds = original_model(images)\n","    unlearned_preds = unlearned_model(images)\n","\n","visualize_multiple_predictions(\n","    images, labels, orig_preds, unlearned_preds,\n","    count=20,\n","    save_dir=\"/content/drive/MyDrive/SafeAI_Final/saved_predictions2\"\n",")\n"],"metadata":{"id":"PynyztQ1kM6R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Fine-Tuning: Enhance Asian Images"],"metadata":{"id":"1juXpqCQ6X1d"}},{"cell_type":"code","source":["model = EyeCNN().to(device)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/SafeAI_Final/eye_cnn_unlearned.pth\", map_location=device))\n","model.to(device)"],"metadata":{"id":"IJk7RuYydShN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forget_indices: 이전에 언러닝 대상이었던 동양인 인덱스 리스트\n","# asian_sample_indices: 전체 동양인 인덱스\n","# → retain된 동양인 인덱스 추출\n","asian_retain_indices = asian_holdout_indices\n","\n","# 유효한 인덱스만 필터링\n","valid_asian_retain_indices = [idx for idx in asian_retain_indices if 0 <= idx < len(train_dataset)]\n","\n","# 해당 서브셋 생성\n","asian_retain_subset = Subset(train_dataset, valid_asian_retain_indices)"],"metadata":{"id":"8-08uE09dTa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"[✅ asian_retain_subset 포함 이미지 수]: {len(valid_asian_retain_indices)}\\n\")\n","\n","for i, idx in enumerate(valid_asian_retain_indices):\n","    image_path = train_df.iloc[idx]['image_path']\n","    print(f\"{i+1:>2}. Index: {idx}, File: {os.path.basename(image_path)}\")"],"metadata":{"id":"Y_c4oe7yj9mx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class WeightedEyeDataset(torch.utils.data.Dataset):\n","    def __init__(self, base_dataset, weight=2.0):\n","        self.base = base_dataset\n","        self.weight = weight\n","\n","    def __len__(self):\n","        return len(self.base)\n","\n","    def __getitem__(self, idx):\n","        image, label = self.base[idx]\n","        return image, label, self.weight\n"],"metadata":{"id":"0XfwxuqUffXr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","\n","def finetune_on_asian_retain(model, dataset, lr=1e-4, epochs=5, batch_size=32):\n","    print(\"[Finetuning] Asian retain weight (restore best)\")\n","    model.train()\n","\n","    weighted_dataset = WeightedEyeDataset(dataset, weight=2.0)\n","    loader = DataLoader(weighted_dataset, batch_size=batch_size, shuffle=True)\n","\n","    optimizer  = torch.optim.Adam(model.parameters(), lr=lr)\n","    best_loss  = float(\"inf\")\n","    best_state = copy.deepcopy(model.state_dict())\n","\n","    for epoch in range(epochs):\n","        epoch_loss = 0.0\n","\n","        for images, targets, weights in loader:\n","            images, targets = images.to(device), targets.to(device)\n","            weights = weights.to(device)\n","\n","            optimizer.zero_grad()\n","            preds = model(images)\n","\n","            loss = F.mse_loss(preds, targets, reduction=\"none\")\n","            loss = (loss.mean(dim=1) * weights).mean()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","\n","        avg_loss = epoch_loss / len(loader)\n","        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n","\n","        if avg_loss < best_loss:\n","            best_loss  = avg_loss\n","            best_state = copy.deepcopy(model.state_dict())\n","\n","    model.load_state_dict(best_state)\n","    print(f\"Fin (best loss = {best_loss:.4f})\")\n","    return model"],"metadata":{"id":"ubhySNsEdTXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = finetune_on_asian_retain(model, asian_retain_subset, lr=1e-4, epochs=50)\n"],"metadata":{"id":"xakV9ib1dTUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/SafeAI_Final/eye_cnn_refined.pth\")\n","print(\"Saved: eye_cnn_refined.pth\")"],"metadata":{"id":"R8GFMYildTSQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visualize"],"metadata":{"id":"Rr2c0BOX5mOA"}},{"cell_type":"code","source":["def visualize_triplet_predictions(images, labels, preds_orig, preds_unl, preds_ref, count=5, save_dir=\"triplet_outputs\"):\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    images = images[:count].cpu()\n","    labels = labels[:count].cpu().numpy()\n","    preds_orig = preds_orig[:count].cpu().numpy()\n","    preds_unl = preds_unl[:count].cpu().numpy()\n","    preds_ref = preds_ref[:count].cpu().numpy()\n","\n","    for i in range(count):\n","        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n","        img = images[i].squeeze(0).numpy()\n","\n","        # GT 좌표\n","        lx_gt, ly_gt, rx_gt, ry_gt = labels[i]\n","\n","        for ax, title, pred in zip(\n","            axes,\n","            [\"Original\", \"Unlearned\", \"Refined\"],\n","            [preds_orig[i], preds_unl[i], preds_ref[i]]\n","        ):\n","            lx_p, ly_p, rx_p, ry_p = pred\n","            ax.imshow(img, cmap=\"gray\")\n","            ax.set_title(title)\n","            ax.axis(\"off\")\n","            ax.scatter([lx_gt, rx_gt], [ly_gt, ry_gt], c=\"blue\", s=30, label=\"GT\")\n","            ax.scatter([lx_p, rx_p], [ly_p, ry_p], c=\"red\", marker=\"x\", s=40, label=\"Pred\")\n","            ax.plot([lx_gt, lx_p], [ly_gt, ly_p], \"--\", c=\"red\", lw=1)\n","            ax.plot([rx_gt, rx_p], [ry_gt, ry_p], \"--\", c=\"red\", lw=1)\n","\n","            err_l = np.linalg.norm([lx_p - lx_gt, ly_p - ly_gt])\n","            err_r = np.linalg.norm([rx_p - rx_gt, ry_p - ry_gt])\n","            ax.text(lx_p + 5, ly_p, f\"{err_l:.1f}px\", color=\"red\", fontsize=8)\n","            ax.text(rx_p + 5, ry_p, f\"{err_r:.1f}px\", color=\"red\", fontsize=8)\n","\n","        handles, labels_ = axes[0].get_legend_handles_labels()\n","        fig.legend(handles, labels_, loc=\"upper center\", ncol=3)\n","        plt.tight_layout()\n","\n","        save_path = os.path.join(save_dir, f\"triplet_{i}.png\")\n","        plt.savefig(save_path, bbox_inches=\"tight\")\n","        plt.close()\n","        print(f\"[✓] 저장됨: {save_path}\")"],"metadata":{"id":"RvKNZXW3hkVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["refined_model = EyeCNN().to(device)\n","refined_model.load_state_dict(torch.load(\"eye_cnn_refined.pth\", map_location=device))\n","refined_model.to(device)\n","\n","data_iter = iter(test_asian_loader)\n","images, labels = next(data_iter)\n","images, labels = images.to(device), labels.to(device)\n","\n","with torch.no_grad():\n","    preds_orig = original_model(images)\n","    preds_unl  = unlearned_model(images)\n","    preds_ref  = refined_model(images)\n","\n","visualize_triplet_predictions(\n","    images, labels,\n","    preds_orig, preds_unl, preds_ref,\n","    count=20,\n","    save_dir=\"/content/drive/MyDrive/SafeAI_Final/triplet_predictions\"\n",")\n"],"metadata":{"id":"K964KceihkNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LPZcsdKMhkEP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Black-Box"],"metadata":{"id":"P__C1SIf6ne-"}},{"cell_type":"code","source":["def blackbox(images, labels, preds_orig, preds_unl, preds_ref, count=5, save_dir=\"triplet_outputs\"):\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    images = images[:count].cpu()\n","    labels = labels[:count].cpu().numpy()\n","    preds_orig = preds_orig[:count].cpu().numpy()\n","    preds_unl = preds_unl[:count].cpu().numpy()\n","    preds_ref = preds_ref[:count].cpu().numpy()\n","\n","    for i in range(count):\n","        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n","        img = images[i].squeeze(0).numpy()\n","\n","        # GT 좌표\n","        lx_gt, ly_gt, rx_gt, ry_gt = labels[i]\n","\n","        for ax, title, pred in zip(\n","            axes,\n","            [\"Original\", \"Unlearned\", \"Refined\"],\n","            [preds_orig[i], preds_unl[i], preds_ref[i]]\n","        ):\n","            lx_p, ly_p, rx_p, ry_p = pred\n","            ax.imshow(img, cmap=\"gray\")\n","            ax.set_title(title)\n","            ax.axis(\"off\")\n","\n","            ax.scatter([lx_gt, rx_gt], [ly_gt, ry_gt], c=\"blue\", s=30, label=\"GT\")\n","            ax.scatter([lx_p, rx_p], [ly_p, ry_p], c=\"red\", marker=\"x\", s=40, label=\"Pred\")\n","\n","            ax.plot([lx_p, rx_p], [ly_p, ry_p], \"-\", color=\"black\", linewidth=14)\n","\n","            err_l = np.linalg.norm([lx_p - lx_gt, ly_p - ly_gt])\n","            err_r = np.linalg.norm([rx_p - rx_gt, ry_p - ry_gt])\n","            ax.text(lx_p + 5, ly_p, f\"{err_l:.1f}px\", color=\"red\", fontsize=8)\n","            ax.text(rx_p + 5, ry_p, f\"{err_r:.1f}px\", color=\"red\", fontsize=8)\n","\n","        handles, labels_ = axes[0].get_legend_handles_labels()\n","        fig.legend(handles, labels_, loc=\"upper center\", ncol=3)\n","        plt.tight_layout()\n","\n","        save_path = os.path.join(save_dir, f\"triplet_{i}.png\")\n","        plt.savefig(save_path, bbox_inches=\"tight\")\n","        plt.close()\n","        print(f\"[✓] 저장됨: {save_path}\")\n"],"metadata":{"id":"RIsiLFlh7rBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["refined_model = EyeCNN().to(device)\n","refined_model.load_state_dict(torch.load(\"eye_cnn_refined.pth\", map_location=device))\n","refined_model.to(device)\n","\n","data_iter = iter(test_asian_loader)\n","images, labels = next(data_iter)\n","images, labels = images.to(device), labels.to(device)\n","\n","with torch.no_grad():\n","    preds_orig = original_model(images)\n","    preds_unl  = unlearned_model(images)\n","    preds_ref  = refined_model(images)\n","\n","blackbox(\n","    images, labels,\n","    preds_orig, preds_unl, preds_ref,\n","    count=20,\n","    save_dir=\"/content/drive/MyDrive/SafeAI_Final/blackbox\"\n",")\n"],"metadata":{"id":"rQp6q6j-9w7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QXzCDuo_CEDf"},"execution_count":null,"outputs":[]}]}