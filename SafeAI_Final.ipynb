{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"xtO31s489w7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749622634334,"user_tz":-540,"elapsed":21714,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"3596193c-0ca9-4c9a-f85b-e655a3e3c15a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gms76zDp22nJ"},"outputs":[],"source":["# !cd /content/drive/MyDrive/SafeAI/BioID-FaceDatabase-V1.2.zip\n","# !unzip /content/drive/MyDrive/SafeAI_Final/SafeAI-main/BioID-FaceDatabase-V1.2.zip"]},{"cell_type":"code","source":["!pip check"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fn6Kb9a9fp7q","executionInfo":{"status":"ok","timestamp":1748997419297,"user_tz":-540,"elapsed":1602,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"29b93781-682e-43be-e5b3-10f83791c1e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ipython 7.34.0 requires jedi, which is not installed.\n","torch 2.6.0+cu124 has requirement nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2.\n","torch 2.6.0+cu124 has requirement nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82.\n","torch 2.6.0+cu124 has requirement nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82.\n","torch 2.6.0+cu124 has requirement nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82.\n","torch 2.6.0+cu124 has requirement nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75.\n","torch 2.6.0+cu124 has requirement nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61.\n","torch 2.6.0+cu124 has requirement nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82.\n","torch 2.6.0+cu124 has requirement nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83.\n","torch 2.6.0+cu124 has requirement nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3.\n","torch 2.6.0+cu124 has requirement nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AroWogBVNRj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q dlib opencv-python\n","\n","!wget -O predictor.bz2 http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","!bzip2 -dk predictor.bz2\n","\n","import os\n","os.rename(\"predictor\", \"shape_predictor_68_face_landmarks.dat\")\n","\n","# data.zip\n","from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"id":"kNf8-RBmNRhV","executionInfo":{"status":"ok","timestamp":1749540591426,"user_tz":-540,"elapsed":148885,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"46f2dd13-7e12-4c9e-98fc-5122aee700a4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-06-10 07:27:26--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","Resolving dlib.net (dlib.net)... 107.180.26.78\n","Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 [following]\n","--2025-06-10 07:27:27--  https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","Connecting to dlib.net (dlib.net)|107.180.26.78|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 64040097 (61M)\n","Saving to: ‘predictor.bz2’\n","\n","predictor.bz2       100%[===================>]  61.07M  36.3MB/s    in 1.7s    \n","\n","2025-06-10 07:27:29 (36.3 MB/s) - ‘predictor.bz2’ saved [64040097/64040097]\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-aaab75a1-8ff4-4dce-a881-ad5213027664\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-aaab75a1-8ff4-4dce-a881-ad5213027664\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving data.zip to data.zip\n"]}]},{"cell_type":"code","source":["import zipfile, os\n","\n","zip_path = next(iter(uploaded))\n","extract_dir = \"data\"\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_dir)\n","\n","import cv2\n","import dlib\n","import numpy as np\n","\n","predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","detector = dlib.get_frontal_face_detector()\n","\n","output_dir = \"output_384x286\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","from glob import glob\n","from tqdm import tqdm\n","\n","img_paths = sorted(glob(f\"{extract_dir}/**/*.jpg\", recursive=True))\n","\n","for path in tqdm(img_paths):\n","    name = os.path.splitext(os.path.basename(path))[0]\n","    img = cv2.imread(path)\n","    if img is None:\n","        continue\n","\n","    resized = cv2.resize(img, (384, 286))\n","    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n","\n","    dets = detector(gray, 1)\n","    if len(dets) == 0:\n","        continue\n","\n","    shape = predictor(gray, dets[0])\n","    landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n","\n","    left_eye = landmarks[36:42]\n","    right_eye = landmarks[42:48]\n","\n","    left_center = np.mean(left_eye, axis=0)\n","    right_center = np.mean(right_eye, axis=0)\n","\n","    cv2.imwrite(f\"{output_dir}/{name}.jpg\", gray)\n","    with open(f\"{output_dir}/{name}.eye\", \"w\") as f:\n","        f.write(f\"{left_center[0]:.2f} {left_center[1]:.2f} {right_center[0]:.2f} {right_center[1]:.2f}\\n\")\n","\n","!zip -r output_384x286.zip output_384x286\n","files.download(\"output_384x286.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"x_sH7MKVPIsg","executionInfo":{"status":"error","timestamp":1749540595178,"user_tz":-540,"elapsed":3748,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"b152d109-9038-4c54-fe2f-accc457c375a"},"execution_count":3,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-a45104a2a796>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import os, shutil, cv2\n","from glob import glob\n","\n","scut_dir = \"output_384x286\"\n","bioid_dir = \"/content/drive/MyDrive/BioID-FaceDatabase-V1.2\"\n","merged_dir = \"BioID-multi-race\"\n","\n","os.makedirs(merged_dir, exist_ok=True)\n","\n","bioid_pgms = glob(f\"{bioid_dir}/**/*.pgm\", recursive=True)\n","bioid_eyes = glob(f\"{bioid_dir}/**/*.eye\", recursive=True)\n","\n","for f in bioid_pgms + bioid_eyes:\n","    shutil.copy(f, os.path.join(merged_dir, os.path.basename(f)))\n","\n","scut_jpgs = glob(f\"{scut_dir}/*.jpg\")\n","\n","for jpg_path in scut_jpgs:\n","    base = os.path.splitext(os.path.basename(jpg_path))[0]\n","\n","    img = cv2.imread(jpg_path, cv2.IMREAD_GRAYSCALE)\n","    if img is None:\n","        continue\n","\n","    pgm_out = os.path.join(merged_dir, base + \".pgm\")\n","    cv2.imwrite(pgm_out, img)\n","\n","    eye_src = os.path.join(scut_dir, base + \".eye\")\n","    eye_dst = os.path.join(merged_dir, base + \".eye\")\n","    if os.path.exists(eye_src):\n","        shutil.copy(eye_src, eye_dst)\n","\n","print(f\"Merged: {len(os.listdir(merged_dir))} files in '{merged_dir}/'\")"],"metadata":{"id":"nwrMAvVwRKfz","executionInfo":{"status":"aborted","timestamp":1749540595191,"user_tz":-540,"elapsed":47910,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","import os\n","\n","drive_target_dir = \"/content/drive/MyDrive/BioID-multi-race\"\n","\n","if os.path.exists(drive_target_dir):\n","    shutil.rmtree(drive_target_dir)\n","\n","shutil.copytree(\"BioID-multi-race\", drive_target_dir)\n","\n","print(\"EndSave\")"],"metadata":{"id":"hhbRpXQKRqd6","executionInfo":{"status":"aborted","timestamp":1749540595192,"user_tz":-540,"elapsed":44929,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oWXc67IuTYM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q opencv-python dlib\n","\n","import os, cv2, struct, zipfile\n","import numpy as np\n","import dlib\n","from glob import glob\n","from tqdm import tqdm\n","from google.colab import files\n","\n","\n","# data.zip\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"Nq8d1o4uxJO-","executionInfo":{"status":"error","timestamp":1749554853799,"user_tz":-540,"elapsed":3457,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"1597d702-f885-4933-bd1d-8372d79eb3e0"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-29bf0e46-6dfa-48a7-8be4-3c6f4be9f60d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-29bf0e46-6dfa-48a7-8be4-3c6f4be9f60d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-ac16a71a6059>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# data.zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["zip_filename = next(iter(uploaded))\n","\n","extract_dir = \"data\"\n","with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n","    zip_ref.extractall(extract_dir)\n","\n","predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","detector = dlib.get_frontal_face_detector()\n","\n","def read_scaled_pts(path, img_shape, target_shape=(384, 286)):\n","    with open(path, 'rb') as f:\n","        raw = f.read()\n","    pts = struct.unpack('f' * 172, raw)\n","    pts = np.array(pts).reshape((86, 2))\n","    h0, w0 = img_shape\n","    w1, h1 = target_shape\n","    scale_x, scale_y = w1 / w0, h1 / h0\n","    return pts * [scale_x, scale_y]\n","\n","out_dir = \"output\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","img_paths = sorted(glob(f\"{extract_dir}/*.jpg\"))\n","for img_path in tqdm(img_paths):\n","    name = os.path.splitext(os.path.basename(img_path))[0]\n","    pts_path = os.path.join(extract_dir, name + \".pts\")\n","    if not os.path.exists(pts_path):\n","        continue\n","\n","    img = cv2.imread(img_path)\n","    if img is None:\n","        continue\n","    orig_h, orig_w = img.shape[:2]\n","    resized = cv2.resize(img, (384, 286))\n","    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n","\n","    pts = read_scaled_pts(pts_path, (orig_h, orig_w))\n","    left_eye = pts[36:42]\n","    right_eye = pts[42:48]\n","    left_center = np.mean(left_eye, axis=0)\n","    right_center = np.mean(right_eye, axis=0)\n","\n","    y_diff = left_center[1] - right_center[1]\n","    if abs(y_diff) > 15:\n","        dets = detector(gray, 1)\n","        if len(dets) > 0:\n","            shape = predictor(gray, dets[0])\n","            landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n","            left_center = np.mean(landmarks[36:42], axis=0)\n","            right_center = np.mean(landmarks[42:48], axis=0)\n","\n","    with open(os.path.join(out_dir, name + \".eye\"), \"w\") as f:\n","        f.write(f\"{left_center[0]:.2f} {left_center[1]:.2f} {right_center[0]:.2f} {right_center[1]:.2f}\\n\")\n","\n","    pgm_path = os.path.join(out_dir, name + \".pgm\")\n","    cv2.imwrite(pgm_path, gray)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1iT4nMs9TXyi","executionInfo":{"status":"ok","timestamp":1749541121470,"user_tz":-540,"elapsed":3026,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"67f12aa2-a727-4938-e6c7-093e3a057432"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"S7Nad2PcTXt-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ccMx7U7BNReW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"metadata":{"id":"VZCbG23sYJse","executionInfo":{"status":"ok","timestamp":1749622662532,"user_tz":-540,"elapsed":4622,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# # base_path = \"/content/drive/MyDrive/SafeAI/BioID-FaceDatabase-V1.2\"\n","# base_path = \"/content/drive/MyDrive/BioID-multi-race\"\n","# image_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".pgm\")])\n","# label_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".eye\")])\n","\n","# print(len(image_files))\n","# print(len(label_files))"],"metadata":{"id":"izbzPEhAY4iU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749542850201,"user_tz":-540,"elapsed":122,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"a1e381b9-7afb-47b6-abac-95d83cff8a2a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["2945\n","2945\n"]}]},{"cell_type":"code","source":["base_path = \"/content/drive/MyDrive/BioID-multi-race\"\n","image_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".pgm\")])\n","label_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".eye\")])\n","\n","print(len(image_files))\n","print(len(label_files))\n","\n","data = []\n","for img_file, label_file in zip(image_files, label_files):\n","    img_path = os.path.join(base_path, img_file)\n","    label_path = os.path.join(base_path, label_file)\n","\n","    with open(label_path, \"r\") as f:\n","        lines = f.readlines()\n","        if len(lines) == 1:\n","            lx, ly, rx, ry = map(float, lines[0].strip().split())\n","        else:\n","          lx, ly, rx, ry = map(float, lines[1].strip().split())\n","        # print(f\"left: {lx}, {ly} / right: {rx}, {ry}\")\n","        data.append((img_path, [lx, ly, rx, ry]))\n"],"metadata":{"id":"3ZrXlwmuY7Kl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749622848192,"user_tz":-540,"elapsed":138070,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"9ad626c5-8076-4dd2-b609-550e37ea1c8c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["2945\n","2945\n"]}]},{"cell_type":"code","source":["len(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"polwZbK03w8R","executionInfo":{"status":"ok","timestamp":1749622848216,"user_tz":-540,"elapsed":7,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"5471f642-9b36-434c-eeb1-906f76c7ef0d"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2945"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["random.seed(42)\n","random.shuffle(data)\n","\n","holdout_data = data[:21]    # 비교용\n","remaining_data = data[21:]  # 학습용\n","\n","df = pd.DataFrame(remaining_data, columns=[\"image_path\", \"eye_coords\"])\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","holdout_df = pd.DataFrame(holdout_data, columns=[\"image_path\", \"eye_coords\"])\n","\n","print(len(remaining_data))"],"metadata":{"id":"kKm_Kj7UtFPA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749622896108,"user_tz":-540,"elapsed":22,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"b8da7263-aede-414e-e683-c46f734ffe32"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["2924\n"]}]},{"cell_type":"code","source":["# import shutil\n","\n","# output_dir = \"/content/drive/MyDrive/SafeAI/comparisons\"\n","# if os.path.exists(output_dir) == False:\n","#   os.makedirs(output_dir, exist_ok=True)\n","\n","#   for img_path in holdout_df[\"image_path\"]:\n","#       base_name = os.path.basename(img_path)\n","#       eye_name = base_name.replace(\".pgm\", \".eye\")\n","#       eye_path = os.path.join(os.path.dirname(img_path), eye_name)\n","\n","#       dst_img_path = os.path.join(output_dir, base_name)\n","#       dst_eye_path = os.path.join(output_dir, eye_name)\n","\n","#       shutil.copy(img_path, dst_img_path)\n","\n","#       if os.path.exists(eye_path):\n","#           shutil.copy(eye_path, dst_eye_path)"],"metadata":{"id":"ggB_Pp0B0Xax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# src_dir = \"/content/drive/MyDrive/SafeAI/BioID-FaceDatabase-V1.2\"\n","# dst_dir = \"/content/drive/MyDrive/SafeAI/comparisons\"\n","\n","# eye_files = [\"BioID_1172.eye\", \"BioID_0331.eye\", \"BioID_0250.eye\"]\n","\n","# os.makedirs(dst_dir, exist_ok=True)\n","\n","# for filename in eye_files:\n","#     src_path = os.path.join(src_dir, filename)\n","#     dst_path = os.path.join(dst_dir, filename)\n","\n","#     try:\n","#         shutil.copy(src_path, dst_path)\n","#         print(f\"복사 완료: {filename}\")\n","#     except Exception as e:\n","#         print(f\"복사 실패: {filename} - {e}\")"],"metadata":{"id":"z3tJDnR7S5Ji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_image(image_path):\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","    image = image.astype(np.float32) / 255.0\n","    image = np.expand_dims(image, axis=0)  # (1, H, W)\n","    return image  # (1, 286, 384)"],"metadata":{"id":"qtRzm7nW6zx4","executionInfo":{"status":"ok","timestamp":1749622916395,"user_tz":-540,"elapsed":4,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class EyeDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.df = dataframe.reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        image = preprocess_image(row[\"image_path\"])  # (1, 128, 128)\n","        label = np.array(row[\"eye_coords\"], dtype=np.float32)\n","        return torch.tensor(image), torch.tensor(label)"],"metadata":{"id":"183Lw50oYF7g","executionInfo":{"status":"ok","timestamp":1749622917580,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class HoldoutDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.df = dataframe.reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        image = preprocess_image(row[\"image_path\"])  # (1, H, W)\n","        label = np.array(row[\"eye_coords\"], dtype=np.float32)\n","        return torch.tensor(image), torch.tensor(label)"],"metadata":{"id":"mwwAX24oy-Bx","executionInfo":{"status":"ok","timestamp":1749622918764,"user_tz":-540,"elapsed":4,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class EyeCNN(nn.Module):\n","    def __init__(self):\n","        super(EyeCNN, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),  # (286, 384) → (143, 192)\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),  # (143, 192) → (71, 96)\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),  # (71, 96) → (35, 48)\n","\n","            nn.AdaptiveAvgPool2d((4, 6)), # 128 * 4 * 6 = 3072\n","            nn.Flatten(),\n","            nn.Dropout(0.3),\n","            nn.Linear(3072, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 4)  # [lx, ly, rx, ry]\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"QIb5l3zryhF3","executionInfo":{"status":"ok","timestamp":1749626371976,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"mvffs_HBfSg_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749626373365,"user_tz":-540,"elapsed":9,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"1a195c67-01a2-4f06-ab3a-276a5861d01a"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["train_dataset = EyeDataset(train_df)\n","test_dataset = EyeDataset(test_df)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","model = EyeCNN().to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"9_HOiKRCYNrD","executionInfo":{"status":"ok","timestamp":1749626374157,"user_tz":-540,"elapsed":16,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# 아래부터는 학습"],"metadata":{"id":"S0Nbt1JXpzP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","best_loss = float('inf')\n","best_model_state = None\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        preds = model(images)\n","        loss = criterion(preds, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n","\n","    if avg_loss < best_loss:\n","        best_loss = avg_loss\n","        best_model_state = model.state_dict()\n","\n","\n","model.load_state_dict(best_model_state)"],"metadata":{"id":"p24Debyv6zt_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749628442521,"user_tz":-540,"elapsed":2065013,"user":{"displayName":"박준영","userId":"01384851238859141058"}},"outputId":"47dd30a7-eb65-4f08-b39d-44c4259217cd"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100] - Loss: 3652.2826\n","Epoch [2/100] - Loss: 373.8912\n","Epoch [3/100] - Loss: 303.7016\n","Epoch [4/100] - Loss: 281.7010\n","Epoch [5/100] - Loss: 254.0284\n","Epoch [6/100] - Loss: 241.9378\n","Epoch [7/100] - Loss: 231.3098\n","Epoch [8/100] - Loss: 209.8149\n","Epoch [9/100] - Loss: 199.2652\n","Epoch [10/100] - Loss: 194.1430\n","Epoch [11/100] - Loss: 166.3980\n","Epoch [12/100] - Loss: 161.1444\n","Epoch [13/100] - Loss: 156.9005\n","Epoch [14/100] - Loss: 141.5475\n","Epoch [15/100] - Loss: 128.3460\n","Epoch [16/100] - Loss: 127.9983\n","Epoch [17/100] - Loss: 122.7501\n","Epoch [18/100] - Loss: 114.0610\n","Epoch [19/100] - Loss: 109.2723\n","Epoch [20/100] - Loss: 103.1062\n","Epoch [21/100] - Loss: 102.7997\n","Epoch [22/100] - Loss: 102.4666\n","Epoch [23/100] - Loss: 97.6750\n","Epoch [24/100] - Loss: 88.4747\n","Epoch [25/100] - Loss: 91.6818\n","Epoch [26/100] - Loss: 89.9264\n","Epoch [27/100] - Loss: 85.2984\n","Epoch [28/100] - Loss: 85.7816\n","Epoch [29/100] - Loss: 82.2579\n","Epoch [30/100] - Loss: 79.5443\n","Epoch [31/100] - Loss: 79.2179\n","Epoch [32/100] - Loss: 77.9501\n","Epoch [33/100] - Loss: 79.1239\n","Epoch [34/100] - Loss: 76.9895\n","Epoch [35/100] - Loss: 77.5727\n","Epoch [36/100] - Loss: 71.0273\n","Epoch [37/100] - Loss: 75.4539\n","Epoch [38/100] - Loss: 70.0758\n","Epoch [39/100] - Loss: 70.9195\n","Epoch [40/100] - Loss: 72.3791\n","Epoch [41/100] - Loss: 73.3218\n","Epoch [42/100] - Loss: 63.4749\n","Epoch [43/100] - Loss: 68.1557\n","Epoch [44/100] - Loss: 70.7316\n","Epoch [45/100] - Loss: 62.0277\n","Epoch [46/100] - Loss: 62.4283\n","Epoch [47/100] - Loss: 65.5297\n","Epoch [48/100] - Loss: 64.6339\n","Epoch [49/100] - Loss: 65.7994\n","Epoch [50/100] - Loss: 65.7511\n","Epoch [51/100] - Loss: 63.1841\n","Epoch [52/100] - Loss: 56.3673\n","Epoch [53/100] - Loss: 58.3687\n","Epoch [54/100] - Loss: 62.4210\n","Epoch [55/100] - Loss: 61.0112\n","Epoch [56/100] - Loss: 55.6085\n","Epoch [57/100] - Loss: 55.9253\n","Epoch [58/100] - Loss: 60.3663\n","Epoch [59/100] - Loss: 54.6478\n","Epoch [60/100] - Loss: 54.2017\n","Epoch [61/100] - Loss: 55.7612\n","Epoch [62/100] - Loss: 54.1285\n","Epoch [63/100] - Loss: 51.9341\n","Epoch [64/100] - Loss: 54.4348\n","Epoch [65/100] - Loss: 53.1290\n","Epoch [66/100] - Loss: 49.9656\n","Epoch [67/100] - Loss: 52.1078\n","Epoch [68/100] - Loss: 52.7690\n","Epoch [69/100] - Loss: 50.4504\n","Epoch [70/100] - Loss: 47.6289\n","Epoch [71/100] - Loss: 48.5068\n","Epoch [72/100] - Loss: 50.0288\n","Epoch [73/100] - Loss: 52.0171\n","Epoch [74/100] - Loss: 49.1687\n","Epoch [75/100] - Loss: 48.5234\n","Epoch [76/100] - Loss: 50.2753\n","Epoch [77/100] - Loss: 48.0315\n","Epoch [78/100] - Loss: 45.5827\n","Epoch [79/100] - Loss: 44.7543\n","Epoch [80/100] - Loss: 44.3722\n","Epoch [81/100] - Loss: 42.6162\n","Epoch [82/100] - Loss: 42.9673\n","Epoch [83/100] - Loss: 44.5042\n","Epoch [84/100] - Loss: 44.0410\n","Epoch [85/100] - Loss: 43.0759\n","Epoch [86/100] - Loss: 42.4128\n","Epoch [87/100] - Loss: 42.3378\n","Epoch [88/100] - Loss: 43.7315\n","Epoch [89/100] - Loss: 52.0757\n","Epoch [90/100] - Loss: 44.6041\n","Epoch [91/100] - Loss: 44.5549\n","Epoch [92/100] - Loss: 45.2623\n","Epoch [93/100] - Loss: 43.1716\n","Epoch [94/100] - Loss: 43.6095\n","Epoch [95/100] - Loss: 41.2033\n","Epoch [96/100] - Loss: 39.0327\n","Epoch [97/100] - Loss: 42.7607\n","Epoch [98/100] - Loss: 39.3802\n","Epoch [99/100] - Loss: 39.0783\n","Epoch [100/100] - Loss: 40.0849\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["print(best_loss)"],"metadata":{"id":"6a8FKoh2txT6","executionInfo":{"status":"aborted","timestamp":1749626369302,"user_tz":-540,"elapsed":652711,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, dataset, device):\n","    model.eval()\n","    predictions = []\n","\n","    for i in range(len(dataset)):\n","        image_tensor, _ = dataset[i]\n","        image_input = image_tensor.unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            pred = model(image_input).cpu().numpy()[0]\n","        predictions.append(pred.tolist())\n","\n","    return predictions"],"metadata":{"id":"7KhDGWeb4hFw","executionInfo":{"status":"aborted","timestamp":1749626369319,"user_tz":-540,"elapsed":651164,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_cnn_predictions(dataset, predictions, num_samples=21):\n","    data_pairs = []\n","\n","    for i in range(min(num_samples, len(dataset))):\n","        _, label = dataset[i]\n","        pred = predictions[i]\n","\n","        gt = label.numpy().tolist()\n","        pr = np.round(pred, 2).tolist()\n","\n","        print(f\"샘플 {i+1}:\")\n","        print(f\" - 정답 좌표 (GT):     {label.numpy().tolist()}\")\n","        print(f\" - 예측 좌표 (CNN):    {np.round(pred, 2).tolist()}\")\n","        print()\n","\n","        data_pairs.append((gt, pr))\n","\n","    mae_list, mse_list, euclidean_list = [], [], []\n","\n","    for gt, pr in data_pairs:\n","        gt = np.array(gt)\n","        pr = np.array(pr)\n","        diff = gt - pr\n","\n","        mae = np.mean(np.abs(diff))\n","        mse = np.mean(diff ** 2)\n","        euclidean = np.linalg.norm(diff)\n","\n","        mae_list.append(mae)\n","        mse_list.append(mse)\n","        euclidean_list.append(euclidean)\n","\n","    print(f\"MAE: {np.mean(mae_list):.2f}\")\n","    print(f\"MSE: {np.mean(mse_list):.2f}\")\n","    print(f\"Euclidean: {np.mean(euclidean_list):.2f}\")"],"metadata":{"id":"qtWCgn-24gzZ","executionInfo":{"status":"aborted","timestamp":1749626369322,"user_tz":-540,"elapsed":650344,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_predictions(dataset, predictions, num_samples=21, cols=3):\n","    rows = (num_samples + cols - 1) // cols\n","    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n","\n","    for i in range(min(num_samples, len(dataset))):\n","        img_tensor, label = dataset[i]\n","        img = img_tensor.numpy()[0]\n","        img = np.squeeze(img)\n","        pred = predictions[i]\n","\n","        ax = axes[i // cols][i % cols] if rows > 1 else axes[i]\n","        ax.imshow(img, cmap='gray')\n","        ax.scatter(label[0], label[1], color='blue', label='GT Left')\n","        ax.scatter(label[2], label[3], color='blue', label='GT Right')\n","        ax.scatter(pred[0], pred[1], color='red', marker='x', label='Pred Left')\n","        ax.scatter(pred[2], pred[3], color='red', marker='x', label='Pred Right')\n","        ax.axis('off')\n","        if i == 0:\n","            ax.legend(loc='upper right')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"dx9wm5WM49vm","executionInfo":{"status":"aborted","timestamp":1749626369324,"user_tz":-540,"elapsed":648986,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["holdout_dataset = HoldoutDataset(holdout_df)"],"metadata":{"id":"7tbMSd385J8_","executionInfo":{"status":"aborted","timestamp":1749626369326,"user_tz":-540,"elapsed":647131,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = predict(model, holdout_dataset, device)"],"metadata":{"id":"xWEAgESZ5Dm9","executionInfo":{"status":"aborted","timestamp":1749626369328,"user_tz":-540,"elapsed":646404,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_cnn_predictions(holdout_dataset, predictions)"],"metadata":{"id":"Pq378O4c5DgP","executionInfo":{"status":"aborted","timestamp":1749626369365,"user_tz":-540,"elapsed":645808,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_predictions(holdout_dataset, predictions, 21, 3)"],"metadata":{"id":"DhesTDY15DW_","executionInfo":{"status":"aborted","timestamp":1749626369367,"user_tz":-540,"elapsed":643989,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/SafeAI_Final/eye_cnn_100epochs.pth\")"],"metadata":{"id":"9yCEIDishjhg","executionInfo":{"status":"aborted","timestamp":1749626369368,"user_tz":-540,"elapsed":639399,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GVc23Gy95RUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vkp8NI_v5Q5V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wmuhvLzi5Q20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 아래부터 Machine Unlearning"],"metadata":{"id":"3cbqQpPDp2Br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습한 모델 로드\n","\n","loaded_model = EyeCNN()\n","loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/SafeAI_Final/eye_cnn_100epochs.pth\"))\n","loaded_model.eval()"],"metadata":{"id":"vCVI4cNC0fC4","executionInfo":{"status":"aborted","timestamp":1749626369369,"user_tz":-540,"elapsed":637681,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 동양인 이미지 찾기\n","\n","base_path = \"/content/drive/MyDrive/BioID-multi-race\"\n","image_files = sorted([f for f in os.listdir(base_path) if f.endswith(\".pgm\")])\n","\n","all_asian_indices = []\n","for idx, img_file in enumerate(image_files):\n","    if 'AM' in img_file or 'AF' in img_file:\n","        all_asian_indices.append(idx)\n","\n","print(f\"전체 동양인 이미지 개수: {len(all_asian_indices)}\")\n","print(f\"전체 이미지 개수: {len(image_files)}\")\n","\n","asian_sample_indices = []\n","\n","for train_idx, row in train_df.iterrows():\n","    image_name = os.path.basename(row[\"image_path\"])\n","    if 'AM' in image_name or 'AF' in image_name:\n","        asian_sample_indices.append(train_idx)\n","\n","print(f\"train_df에서 동양인 이미지 개수: {len(asian_sample_indices)}\")\n","print(f\"train_df 총 개수: {len(train_df)}\")\n","\n","train_df = train_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","train_dataset = EyeDataset(train_df)\n","test_dataset = EyeDataset(test_df)\n","\n","asian_sample_indices = []\n","for idx, row in train_df.iterrows():\n","    image_name = os.path.basename(row[\"image_path\"])\n","    if 'AM' in image_name or 'AF' in image_name:\n","        asian_sample_indices.append(idx)\n","\n","print(f\"train_dataset 길이: {len(train_dataset)}\")\n","print(f\"asian_sample_indices 최대값: {max(asian_sample_indices) if asian_sample_indices else 'None'}\")\n","print(f\"asian_sample_indices 개수: {len(asian_sample_indices)}\")\n","\n","valid_indices = [idx for idx in asian_sample_indices if 0 <= idx < len(train_dataset)]\n","print(f\"유효한 인덱스 개수: {len(valid_indices)}\")\n"],"metadata":{"id":"iKuIQe6c0kjN","executionInfo":{"status":"aborted","timestamp":1749626369370,"user_tz":-540,"elapsed":636030,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 삭제하고싶은 동양인 비율\n","\n"],"metadata":{"id":"Yu_OLPc62-G7","executionInfo":{"status":"aborted","timestamp":1749626369371,"user_tz":-540,"elapsed":633960,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Subset\n","import time"],"metadata":{"id":"2jzcnk9O5ey4","executionInfo":{"status":"aborted","timestamp":1749626369375,"user_tz":-540,"elapsed":633103,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def machine_unlearning_gradient_ascent(model, forget_loader, retain_loader, lr=1e-5, epochs=5):\n","    \"\"\"Gradient ascent unlearning with debugging\"\"\"\n","    print(f\"[Stage 1] Gradient Ascent Unlearning\")\n","    print(f\"Learning rate: {lr}, Epochs: {epochs}\")\n","    print(f\"Forget samples: {len(forget_loader.dataset)}, Retain samples: {len(retain_loader.dataset)}\")\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_forget_loss = 0\n","        epoch_retain_loss = 0\n","\n","        # Forget phase\n","        for batch_idx, (data, targets) in enumerate(forget_loader):\n","            # 🐛 디버깅: 첫 배치 확인\n","            if epoch == 0 and batch_idx == 0:\n","                print(f\"[DEBUG] Batch 0 - data shape: {data.shape}, targets shape: {targets.shape}\")\n","                print(f\"[DEBUG] Data has nan?: {torch.isnan(data).any().item()}, inf?: {torch.isinf(data).any().item()}\")\n","                print(f\"[DEBUG] Targets has nan?: {torch.isnan(targets).any().item()}, inf?: {torch.isinf(targets).any().item()}\")\n","\n","                with torch.no_grad():\n","                    output = model(data)\n","                    print(f\"[DEBUG] Output min: {output.min().item()}, max: {output.max().item()}\")\n","                    print(f\"[DEBUG] Output has nan?: {torch.isnan(output).any().item()}, inf?: {torch.isinf(output).any().item()}\")\n","\n","                    try:\n","                        debug_loss = F.mse_loss(output, targets)\n","                        print(f\"[DEBUG] MSE loss: {debug_loss.item()}\")\n","                    except Exception as e:\n","                        print(f\"[DEBUG] Loss computation error: {e}\")\n","\n","            model.zero_grad()\n","            predictions = model(data)\n","            forget_loss = F.mse_loss(predictions, targets)\n","            forget_loss.backward()\n","\n","            # Gradient ascent 적용 (optimizer 사용 안함)\n","            for param in model.parameters():\n","                if param.grad is not None:\n","                    param.data += lr * param.grad\n","\n","            epoch_forget_loss += forget_loss.item()\n","\n","        # Retain phase (기존 방식 그대로)\n","        for batch_idx, (data, targets) in enumerate(retain_loader):\n","            model.zero_grad()\n","            predictions = model(data)\n","            retain_loss = F.mse_loss(predictions, targets)\n","            retain_loss.backward()\n","            for param in model.parameters():\n","                if param.grad is not None:\n","                    param.data -= lr * param.grad  # gradient descent\n","            epoch_retain_loss += retain_loss.item()\n","\n","        avg_forget = epoch_forget_loss / len(forget_loader)\n","        avg_retain = epoch_retain_loss / len(retain_loader)\n","        print(f\"Epoch {epoch+1}: Forget Loss {avg_forget:.6f}, Retain Loss {avg_retain:.6f}\")\n","\n","    print(f\"Stage 1 completed\\n\")\n","    return model"],"metadata":{"id":"Y_nVw8TT8sjm","executionInfo":{"status":"aborted","timestamp":1749626369379,"user_tz":-540,"elapsed":630423,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adaptive_unlearning(model, asian_loader, western_loader):\n","    \"\"\"Adaptive parameter adjustment with minimal logging\"\"\"\n","    print(f\"[Stage 2] Adaptive Parameter Adjustment\")\n","\n","    print(\"Computing Fisher Information...\")\n","    fisher_asian = compute_fisher_information(model, asian_loader)\n","    fisher_western = compute_fisher_information(model, western_loader)\n","\n","    print(\"Calculating differential importance...\")\n","    differential_importance = {}\n","    total_params = 0\n","    adjusted_params = 0\n","\n","    for name in fisher_asian:\n","        differential_importance[name] = fisher_asian[name] - fisher_western[name]\n","        total_params += fisher_asian[name].numel()\n","\n","    print(\"Adjusting parameters...\")\n","    for name, param in model.named_parameters():\n","        if name in differential_importance:\n","            importance_diff = differential_importance[name]\n","            adjustment_mask = importance_diff > 0\n","            adjusted_count = adjustment_mask.sum().item()\n","            adjusted_params += adjusted_count\n","\n","            with torch.no_grad():\n","                param[adjustment_mask] *= 0.1\n","\n","    print(f\"Adjusted {adjusted_params}/{total_params} parameters ({adjusted_params/total_params*100:.1f}%)\")\n","    print(f\"Stage 2 completed\\n\")\n","\n","    return model\n"],"metadata":{"id":"YdKJjv5g8shI","executionInfo":{"status":"aborted","timestamp":1749626369380,"user_tz":-540,"elapsed":627259,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_fisher_information(model, data_loader):\n","    \"\"\"Compute Fisher Information Matrix\"\"\"\n","    fisher_dict = {}\n","    model.eval()\n","\n","    for name, param in model.named_parameters():\n","        fisher_dict[name] = torch.zeros_like(param)\n","\n","    for batch_idx, (data, targets) in enumerate(data_loader):\n","        model.zero_grad()\n","        output = model(data)\n","        loss = F.mse_loss(output, targets)\n","        loss.backward()\n","\n","        for name, param in model.named_parameters():\n","            if param.grad is not None:\n","                fisher_dict[name] += param.grad.pow(2)\n","\n","    for name in fisher_dict:\n","        fisher_dict[name] /= len(data_loader)\n","\n","    return fisher_dict\n"],"metadata":{"id":"zMCVoAcQ8sey","executionInfo":{"status":"aborted","timestamp":1749626369382,"user_tz":-540,"elapsed":625477,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def neuron_level_unlearning(model, asian_indices, prune_ratio=0.2):\n","    \"\"\"Structural pruning with minimal logging\"\"\"\n","    print(f\"[Stage 3] Structural Pruning\")\n","    print(f\"Pruning ratio: {prune_ratio*100:.1f}%\")\n","\n","    import torch.nn.utils.prune as prune\n","\n","    pruned_layers = 0\n","    total_params_before = sum(p.numel() for p in model.parameters())\n","\n","    for name, module in model.named_modules():\n","        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n","            prune.l1_unstructured(module, name='weight', amount=prune_ratio)\n","            prune.remove(module, 'weight')\n","            pruned_layers += 1\n","\n","    total_params_after = sum(p.numel() for p in model.parameters())\n","    reduction_rate = (total_params_before-total_params_after)/total_params_before*100\n","\n","    print(f\"Pruned {pruned_layers} layers\")\n","    print(f\"Parameters: {total_params_before} -> {total_params_after} ({reduction_rate:.1f}% reduction)\")\n","    print(f\"Stage 3 completed\\n\")\n","\n","    return model\n"],"metadata":{"id":"k3JsFLni8scO","executionInfo":{"status":"aborted","timestamp":1749626369383,"user_tz":-540,"elapsed":623665,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def complete_unlearning_pipeline(model, dataset, asian_indices):\n","    \"\"\"Complete machine unlearning pipeline with minimal logging\"\"\"\n","    print(f\"Machine Unlearning Pipeline Started\")\n","    print(f\"Total data: {len(dataset)}, Asian samples: {len(asian_indices)}\")\n","    print(f\"Removal ratio: {len(asian_indices)/len(dataset)*100:.1f}%\\n\")\n","\n","    # Data splitting\n","    asian_data = Subset(dataset, asian_indices)\n","    western_indices = [i for i in range(len(dataset)) if i not in asian_indices]\n","    western_data = Subset(dataset, western_indices)\n","\n","    asian_loader = DataLoader(asian_data, batch_size=32, shuffle=True)\n","    western_loader = DataLoader(western_data, batch_size=32, shuffle=True)\n","\n","    # Stage 1: Gradient ascent unlearning\n","    model = machine_unlearning_gradient_ascent(model, asian_loader, western_loader, epochs=5)\n","\n","    # Stage 2: Adaptive parameter adjustment\n","    model = adaptive_unlearning(model, asian_loader, western_loader)\n","\n","    # Stage 3: Structural pruning\n","    model = neuron_level_unlearning(model, asian_indices)\n","\n","    print(f\"Machine Unlearning Pipeline Completed\")\n","    print(f\"Processed {len(asian_indices)} Asian samples\")\n","\n","    return model\n"],"metadata":{"id":"bCZEuu7G82N-","executionInfo":{"status":"aborted","timestamp":1749626369384,"user_tz":-540,"elapsed":622523,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if len(valid_indices) > 0:\n","    unlearned_model = complete_unlearning_pipeline(\n","        loaded_model,\n","        train_dataset,\n","        valid_indices\n","    )\n","    print(\"Success\")\n","else:\n","    print(\"IndexOutofRange\")\n","\n","\n","# 결과 저장\n","torch.save(unlearned_model.state_dict(), 'eye_cnn_unlearned.pth')\n","print(\"Model Saved: 'eye_cnn_unlearned.pth'\")\n"],"metadata":{"id":"GnEZ9FKE2e-J","executionInfo":{"status":"aborted","timestamp":1749626369386,"user_tz":-540,"elapsed":620695,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터에서 동양인/서양인 인덱스 추출\n","test_asian_indices = []\n","test_western_indices = []\n","\n","for idx, row in test_df.iterrows():\n","    image_name = os.path.basename(row[\"image_path\"])\n","    if 'AM' in image_name or 'AF' in image_name:\n","        test_asian_indices.append(idx)\n","    else:\n","        test_western_indices.append(idx)\n","\n","print(f\"테스트 동양인 이미지: {len(test_asian_indices)}개\")\n","print(f\"테스트 서양인 이미지: {len(test_western_indices)}개\")\n"],"metadata":{"id":"6vNHAs_O5w8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Subset, DataLoader\n","\n","# 동양인/서양인 테스트 데이터셋 생성\n","test_asian_dataset = Subset(test_dataset, test_asian_indices)\n","test_western_dataset = Subset(test_dataset, test_western_indices)\n","\n","# 데이터로더 생성\n","test_asian_loader = DataLoader(test_asian_dataset, batch_size=32, shuffle=False)\n","test_western_loader = DataLoader(test_western_dataset, batch_size=32, shuffle=False)\n"],"metadata":{"id":"77j487Fz55j9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원본 모델 로드 (언러닝 전)\n","original_model = torch.load('eye_cnn_100epochs.pth')\n","\n","# 언러닝 효과 평가 실행\n","unlearning_ratio, retention_ratio = evaluate_unlearning_effectiveness(\n","    original_model=original_model,\n","    unlearned_model=unlearned_model,\n","    test_asian_loader=test_asian_loader,\n","    test_western_loader=test_western_loader\n",")\n","\n","print(f\"\\n=== 머신 언러닝 효과 분석 ===\")\n","print(f\"동양인 데이터 언러닝 비율: {unlearning_ratio:.4f}\")\n","print(f\"서양인 데이터 유지 비율: {retention_ratio:.4f}\")\n"],"metadata":{"id":"WTYFzaxb5655"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 결과 해석 함수\n","def interpret_results(unlearning_ratio, retention_ratio):\n","    print(f\"\\n=== 결과 해석 ===\")\n","\n","    if unlearning_ratio > 1.2:\n","        print(\"✅ 동양인 데이터 언러닝 효과 우수 (20% 이상 성능 저하)\")\n","    elif unlearning_ratio > 1.0:\n","        print(\"⚠️ 동양인 데이터 언러닝 효과 보통\")\n","    else:\n","        print(\"❌ 동양인 데이터 언러닝 효과 미흡\")\n","\n","    if 0.9 <= retention_ratio <= 1.1:\n","        print(\"✅ 서양인 데이터 성능 잘 유지됨\")\n","    else:\n","        print(\"⚠️ 서양인 데이터 성능에 영향 있음\")\n","\n","# 결과 해석\n","interpret_results(unlearning_ratio, retention_ratio)\n"],"metadata":{"id":"gf0BUrqt58fN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8g1iCzoy0Ql2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qU1SsVqez6gG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fh89N5hbz6dP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import os\n","import numpy as np\n","\n","def draw_black_boxes_from_predictions(predictions, holdout_df, output_dir, box_height=20):\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    for i, pred in enumerate(predictions):\n","        try:\n","            lx, ly, rx, ry = map(int, pred)\n","            image_path = holdout_df.iloc[i][\"image_path\"]\n","            filename = os.path.basename(image_path)\n","            filename_no_ext = os.path.splitext(filename)[0]\n","            output_path = os.path.join(output_dir, f\"boxed_{filename_no_ext}.jpg\")\n","\n","            image = cv2.imread(image_path)\n","\n","            x_min = max(min(lx, rx) - 20, 0)\n","            x_max = min(max(lx, rx) + 20, image.shape[1] - 1)\n","\n","            y_center = (ly + ry) // 2\n","            y_min = max(y_center - box_height // 2, 0)\n","            y_max = min(y_center + box_height // 2, image.shape[0] - 1)\n","\n","            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 0), thickness=-1)\n","\n","            result = cv2.imwrite(output_path, image)\n","            if result:\n","                print(f\"저장 완료: {output_path}\")\n","            else:\n","                print(f\"저장 실패: {output_path}\")\n","\n","        except Exception as e:\n","            print(f\"Error [{i}]: {e}\")"],"metadata":{"id":"iM-CLXPxh7y5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = predict(model, holdout_dataset, device)\n","\n","draw_black_boxes_from_predictions(\n","    predictions=predictions,\n","    holdout_df=holdout_df,\n","    output_dir=\"/content/drive/MyDrive/SafeAI/boxed_holdout\",\n","    box_height=20\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Js0MkVZhh7ui","executionInfo":{"status":"ok","timestamp":1747559632043,"user_tz":-540,"elapsed":390,"user":{"displayName":"박준영","userId":"03119642008586225166"}},"outputId":"07c6ee74-4029-462e-c0f8-e28b8bf36e2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_1181.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0134.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_1245.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_1115.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0937.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0293.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0921.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_1501.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0182.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0181.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0989.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0729.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0213.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0705.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0645.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0604.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_1374.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_1329.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0214.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_1276.jpg\n","저장 완료: /content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0972.jpg\n"]}]},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","img = cv2.imread(\"/content/drive/MyDrive/SafeAI/boxed_holdout/boxed_BioID_0134.jpg\")\n","plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","plt.axis('off')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"rd_rwXKWiV8J","executionInfo":{"status":"error","timestamp":1747559150172,"user_tz":-540,"elapsed":21,"user":{"displayName":"박준영","userId":"03119642008586225166"}},"outputId":"0555d6d6-79a8-47f6-9413-3cf4ce75863f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"error","evalue":"OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-149-6d0ce7c83fd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/SafeAI/BioID-FaceDatabase-V1.2/boxed_BioID_0007.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/drive/MyDrive/SafeAI_Final/eye_cnn_100epochs.pth\")"],"metadata":{"id":"Oe3J-7R7ahdM","executionInfo":{"status":"ok","timestamp":1749557404582,"user_tz":-540,"elapsed":672,"user":{"displayName":"박준영","userId":"01384851238859141058"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# 불러오기\n","\n","# model = EyeCNN()\n","# model.load_state_dict(torch.load(\"eye_cnn.pth\"))\n","# model.eval()\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)"],"metadata":{"id":"ayJQ9TR8ahbI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xrhGIGmmahYw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### RetinaFace"],"metadata":{"id":"9W5OPkzIq18C"}},{"cell_type":"code","source":["!pip install retina-face opencv-python-headless"],"metadata":{"id":"nAaxYhcmYJeb","executionInfo":{"status":"ok","timestamp":1747556608345,"user_tz":-540,"elapsed":2055,"user":{"displayName":"박준영","userId":"03119642008586225166"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c60c962b-8645-4e00-ccae-601e8a483e48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: retina-face in /usr/local/lib/python3.11/dist-packages (0.0.17)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from retina-face) (2.0.2)\n","Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from retina-face) (5.2.0)\n","Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from retina-face) (11.2.1)\n","Requirement already satisfied: opencv-python>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from retina-face) (4.11.0.86)\n","Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from retina-face) (2.18.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->retina-face) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->retina-face) (3.18.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->retina-face) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->retina-face) (4.67.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (5.29.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (4.13.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (3.13.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->retina-face) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.0.9)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.15.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2025.4.26)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->retina-face) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->retina-face) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->retina-face) (3.1.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->retina-face) (2.7)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (1.7.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=1.9.0->retina-face) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.1.2)\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from retinaface import RetinaFace\n","\n","def compare_eye_coords(input_dir):\n","    files = sorted([f for f in os.listdir(input_dir) if f.lower().endswith(\".pgm\")])\n","    success_count = 0\n","    fail_count = 0\n","\n","    mae_list, mse_list, euclidean_list = [], [], []\n","    data_pairs = []\n","\n","    def read_eye_file(eye_path):\n","        try:\n","            with open(eye_path, 'r') as f:\n","                lines = f.readlines()\n","                if len(lines) < 2:\n","                    return None\n","                values = lines[1].strip().split()\n","                if len(values) != 4:\n","                    return None\n","                lx, ly, rx, ry = map(int, values)\n","                return (lx, ly), (rx, ry)\n","        except Exception as e:\n","            print(f\"file error: {eye_path} - {e}\")\n","            return None\n","\n","    print(\"filename\\tGT_L\\tGT_R\\tPR_L\\tPR_R\\tL_error\\tR_error\")\n","\n","    for filename in files:\n","        image_path = os.path.join(input_dir, filename)\n","        eye_path = image_path.replace(\".pgm\", \".eye\")\n","\n","        gt_coords = read_eye_file(eye_path)\n","        gt_left, gt_right = gt_coords\n","\n","        gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","        rgb = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n","\n","        try:\n","            faces = RetinaFace.detect_faces(rgb)\n","            if not faces:\n","                print(f\"No face: {filename}\")\n","                fail_count += 1\n","                continue\n","\n","            face = list(faces.values())[0]\n","            landmarks = face[\"landmarks\"]\n","            pr_left = landmarks[\"left_eye\"]\n","            pr_right = landmarks[\"right_eye\"]\n","\n","            l_error = round(np.linalg.norm(np.array(gt_left) - np.array(pr_left)), 2)\n","            r_error = round(np.linalg.norm(np.array(gt_right) - np.array(pr_right)), 2)\n","\n","            print(f\"{filename}\\t{gt_left}\\t{gt_right}\\t{pr_left}\\t{pr_right}\\t{l_error}\\t{r_error}\")\n","            success_count += 1\n","\n","            # 전체 지표를 위한 누적\n","            gt = np.array([*gt_left, *gt_right])\n","            pr = np.array([*pr_left, *pr_right])\n","            data_pairs.append((gt, pr))\n","\n","        except Exception as e:\n","            print(f\"Exception Error: {filename} - {type(e).__name__}: {e}\")\n","            fail_count += 1\n","\n","    # 📊 SOTA 지표 계산\n","    for gt, pr in data_pairs:\n","        diff = gt - pr\n","        mae = np.mean(np.abs(diff))\n","        mse = np.mean(diff ** 2)\n","        euclidean = np.linalg.norm(diff)\n","\n","        mae_list.append(mae)\n","        mse_list.append(mse)\n","        euclidean_list.append(euclidean)\n","\n","    print(f\"\\nMAE: {np.mean(mae_list):.2f}\")\n","    print(f\"MSE: {np.mean(mse_list):.2f}\")\n","    print(f\"Euclidean: {np.mean(euclidean_list):.2f}\")"],"metadata":{"id":"baGsznh5Qahi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["compare_eye_coords(\"/content/drive/MyDrive/SafeAI/comparisons\")"],"metadata":{"id":"qE78TyZr6QCH","executionInfo":{"status":"ok","timestamp":1747557814321,"user_tz":-540,"elapsed":3667,"user":{"displayName":"박준영","userId":"03119642008586225166"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"17cea085-600c-4093-d581-1d41ac723507"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["filename\tGT_L\tGT_R\tPR_L\tPR_R\tL_error\tR_error\n","BioID_0140.pgm\t(243, 119)\t(179, 116)\t[np.float32(239.53946), np.float32(114.65381)]\t[np.float32(178.13518), np.float32(114.47114)]\t5.56\t1.76\n","BioID_0207.pgm\t(242, 113)\t(177, 112)\t[np.float32(241.50998), np.float32(110.82748)]\t[np.float32(179.08981), np.float32(111.001785)]\t2.23\t2.32\n","BioID_0211.pgm\t(232, 115)\t(167, 112)\t[np.float32(229.87245), np.float32(112.67509)]\t[np.float32(167.9954), np.float32(109.987915)]\t3.15\t2.24\n","BioID_0250.pgm\t(259, 113)\t(208, 111)\t[np.float32(260.0051), np.float32(112.56578)]\t[np.float32(208.38177), np.float32(109.92682)]\t1.09\t1.14\n","BioID_0331.pgm\t(204, 145)\t(160, 142)\t[np.float32(201.7934), np.float32(144.59041)]\t[np.float32(157.7959), np.float32(142.94089)]\t2.24\t2.4\n","BioID_0400.pgm\t(164, 100)\t(119, 98)\t[np.float32(162.13423), np.float32(100.43073)]\t[np.float32(117.47988), np.float32(99.757996)]\t1.91\t2.32\n","BioID_0446.pgm\t(179, 100)\t(119, 99)\t[np.float32(180.51622), np.float32(101.72968)]\t[np.float32(117.768814), np.float32(99.07033)]\t2.3\t1.23\n","BioID_0629.pgm\t(211, 77)\t(159, 76)\t[np.float32(209.73401), np.float32(75.69925)]\t[np.float32(159.24185), np.float32(75.02224)]\t1.82\t1.01\n","BioID_0653.pgm\t(205, 116)\t(157, 114)\t[np.float32(202.95645), np.float32(114.660805)]\t[np.float32(160.25429), np.float32(111.715775)]\t2.44\t3.98\n","BioID_0741.pgm\t(212, 128)\t(170, 127)\t[np.float32(210.37814), np.float32(125.94176)]\t[np.float32(170.78387), np.float32(125.266106)]\t2.62\t1.9\n","BioID_0881.pgm\t(211, 103)\t(170, 102)\t[np.float32(211.24837), np.float32(100.05015)]\t[np.float32(170.47519), np.float32(100.08951)]\t2.96\t1.97\n","BioID_0990.pgm\t(226, 92)\t(183, 96)\t[np.float32(227.63715), np.float32(90.82859)]\t[np.float32(184.90063), np.float32(94.04318)]\t2.01\t2.73\n","BioID_1112.pgm\t(233, 78)\t(184, 83)\t[np.float32(234.24036), np.float32(75.90603)]\t[np.float32(184.56946), np.float32(81.60126)]\t2.43\t1.51\n","BioID_1121.pgm\t(234, 81)\t(184, 86)\t[np.float32(234.41754), np.float32(79.46231)]\t[np.float32(185.04688), np.float32(84.88721)]\t1.59\t1.53\n","BioID_1134.pgm\t(219, 103)\t(163, 106)\t[np.float32(217.10461), np.float32(102.61308)]\t[np.float32(164.67017), np.float32(107.44161)]\t1.93\t2.21\n","BioID_1148.pgm\t(229, 92)\t(171, 95)\t[np.float32(228.05208), np.float32(91.78401)]\t[np.float32(171.70464), np.float32(95.80165)]\t0.97\t1.07\n","BioID_1162.pgm\t(223, 91)\t(162, 92)\t[np.float32(222.33514), np.float32(93.105034)]\t[np.float32(161.21965), np.float32(93.64053)]\t2.21\t1.82\n","BioID_1172.pgm\t(230, 95)\t(165, 94)\t[np.float32(228.08543), np.float32(90.76988)]\t[np.float32(164.71461), np.float32(91.66931)]\t4.64\t2.35\n","BioID_1220.pgm\t(216, 60)\t(180, 61)\t[np.float32(216.61102), np.float32(58.05588)]\t[np.float32(179.47502), np.float32(60.255028)]\t2.04\t0.91\n","BioID_1295.pgm\t(216, 96)\t(162, 101)\t[np.float32(216.762), np.float32(95.3899)]\t[np.float32(160.31903), np.float32(98.56196)]\t0.98\t2.96\n","BioID_1493.pgm\t(212, 79)\t(129, 80)\t[np.float32(210.34001), np.float32(79.78454)]\t[np.float32(131.38295), np.float32(76.34035)]\t1.84\t4.37\n","\n","MAE: 1.46\n","MSE: 2.92\n","Euclidean: 3.23\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wtIABuUrnAej"},"execution_count":null,"outputs":[]}]}